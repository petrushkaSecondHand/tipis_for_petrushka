{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973af0b9-f6d2-48c2-a6db-948a0a41e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Загрузка данных\n",
    "data = pd.read_csv(r'C:\\Users\\Oleg\\Documents\\bank\\bank-full.csv', delimiter=\";\")\n",
    "\n",
    "# Выбор указанных признаков\n",
    "data_selected = data[['age', 'job', 'marital', 'education', 'balance', 'housing', 'contact',\n",
    "                      'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'y']]\n",
    "\n",
    "# Проверка пропущенных значений\n",
    "print(\"Пропущенные значения:\\n\", data_selected.isnull().sum())\n",
    "\n",
    "# Вопрос 1: Самое частое значение (mode) для столбца education\n",
    "education_mode = data_selected['education'].mode()[0]\n",
    "print(\"Самое частое значение для education:\", education_mode)\n",
    "\n",
    "# Вопрос 2: Корреляционная матрица для числовых признаков\n",
    "numeric_features = data_selected[['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']]\n",
    "correlation_matrix = numeric_features.corr()\n",
    "print(\"Корреляционная матрица:\\n\", correlation_matrix)\n",
    "\n",
    "# Поиск пары признаков с наибольшей корреляцией\n",
    "correlation_pairs = correlation_matrix.unstack()\n",
    "sorted_correlation = correlation_pairs.sort_values(ascending=False)\n",
    "strongest_correlation = sorted_correlation[(sorted_correlation != 1.0)].idxmax()\n",
    "print(\"Наибольшая корреляция между признаками:\", strongest_correlation)\n",
    "\n",
    "# Кодирование целевой переменной y\n",
    "data_selected.loc[:, 'y'] = data_selected['y'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "print(\"Уникальные значения в y после кодирования:\", data_selected['y'].unique())\n",
    "\n",
    "# Разделение данных на обучающую, валидационную и тестовую выборки\n",
    "train_data, temp_data = train_test_split(data_selected, test_size=0.4, random_state=42, stratify=data_selected['y'])\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, stratify=temp_data['y'])\n",
    "\n",
    "# Разделение признаков и целевой переменной\n",
    "X_train = train_data.drop(columns=['y'])\n",
    "y_train = train_data['y']\n",
    "X_val = val_data.drop(columns=['y'])\n",
    "y_val = val_data['y']\n",
    "X_test = test_data.drop(columns=['y'])\n",
    "y_test = test_data['y']\n",
    "\n",
    "# Преобразуем целевые переменные в целочисленный тип данных\n",
    "y_train = y_train.astype(int)\n",
    "y_val = y_val.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "# Вопрос 3: Взаимная информация между y и категориальными переменными\n",
    "categorical_features = ['job', 'marital', 'education', 'housing', 'contact', 'month', 'poutcome']\n",
    "X_train_encoded = pd.get_dummies(X_train, columns=categorical_features)\n",
    "mi_scores = mutual_info_classif(X_train_encoded, y_train, discrete_features=True)\n",
    "mi_results = pd.Series(mi_scores, index=X_train_encoded.columns).round(2)\n",
    "\n",
    "# Отображаем результаты для всех признаков с наибольшей взаимной информацией\n",
    "print(\"Топ взаимной информации для категориальных признаков:\\n\", mi_results.sort_values(ascending=False).head(10))\n",
    "\n",
    "# Вопрос 4: Логистическая регрессия и точность на валидационном наборе\n",
    "X_val_encoded = pd.get_dummies(X_val, columns=categorical_features)\n",
    "X_train_encoded, X_val_encoded = X_train_encoded.align(X_val_encoded, join='left', axis=1, fill_value=0)\n",
    "\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "model.fit(X_train_encoded, y_train)\n",
    "y_val_pred = model.predict(X_val_encoded)\n",
    "\n",
    "# Рассчитываем и выводим точность\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(\"Точность на валидационном наборе данных:\", round(val_accuracy, 2))\n",
    "# Шаг 1: Обучаем модель на всех признаках и вычисляем исходную точность\n",
    "model_full = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "model_full.fit(X_train_encoded, y_train)\n",
    "y_val_pred_full = model_full.predict(X_val_encoded)\n",
    "accuracy_full = accuracy_score(y_val, y_val_pred_full)\n",
    "print(\"Исходная точность модели на всех признаках:\", round(accuracy_full, 2))\n",
    "\n",
    "# Словарь для хранения разницы точности\n",
    "accuracy_diff = {}\n",
    "\n",
    "# Шаг 2: Последовательно исключаем каждый признак и измеряем изменение точности\n",
    "features_to_check = ['age', 'balance', 'marital', 'previous']  # Признаки, указанные в вопросе\n",
    "\n",
    "for feature in features_to_check:\n",
    "    # Исключаем признак из тренировочного и валидационного наборов\n",
    "    X_train_reduced = X_train_encoded.drop(columns=[feature], errors='ignore')\n",
    "    X_val_reduced = X_val_encoded.drop(columns=[feature], errors='ignore')\n",
    "\n",
    "    # Обучаем модель без текущего признака\n",
    "    model_reduced = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "    model_reduced.fit(X_train_reduced, y_train)\n",
    "\n",
    "    # Рассчитываем точность модели без текущего признака\n",
    "    y_val_pred_reduced = model_reduced.predict(X_val_reduced)\n",
    "    accuracy_reduced = accuracy_score(y_val, y_val_pred_reduced)\n",
    "\n",
    "    # Вычисляем разницу в точности и сохраняем результат\n",
    "    accuracy_diff[feature] = accuracy_full - accuracy_reduced\n",
    "    print(f\"Разница в точности при исключении {feature}: {round(accuracy_diff[feature], 4)}\")\n",
    "\n",
    "# Шаг 3: Находим признак с наименьшей разницей в точности\n",
    "least_impact_feature = min(accuracy_diff, key=accuracy_diff.get)\n",
    "print(f\"\\nПризнак с наименьшей разницей в точности: {least_impact_feature}\")\n",
    "# Массив значений параметра C\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Словарь для хранения точностей для каждого значения C\n",
    "accuracy_by_C = {}\n",
    "\n",
    "# Шаг 1: Обучаем модели с разными значениями C и вычисляем точность на валидационном наборе\n",
    "for C in C_values:\n",
    "    # Инициализация модели логистической регрессии с заданным значением C\n",
    "    model = LogisticRegression(solver='liblinear', C=C, max_iter=1000, random_state=42)\n",
    "\n",
    "    # Обучение модели\n",
    "    model.fit(X_train_encoded, y_train)\n",
    "\n",
    "    # Предсказания на валидационном наборе\n",
    "    y_val_pred = model.predict(X_val_encoded)\n",
    "\n",
    "    # Рассчитываем точность и сохраняем результат в словарь\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    accuracy_by_C[C] = round(accuracy, 3)\n",
    "    print(f\"Точность для C={C}: {accuracy_by_C[C]}\")\n",
    "\n",
    "# Шаг 2: Находим значение C с наивысшей точностью (и выбираем наименьшее, если есть несколько максимальных)\n",
    "best_C = min([k for k, v in accuracy_by_C.items() if v == max(accuracy_by_C.values())])\n",
    "print(f\"\\nЗначение C с наилучшей точностью: {best_C}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
